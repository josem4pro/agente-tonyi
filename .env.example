# agente-tonyi â€” Environment Template
# Copy to .env and adjust values for your setup

# === Agent Mode ===
AGENT_MODE=true                    # true=concise answers, false=full research reports

# === Model & Inference ===
LLM_LOCAL_PATH=/path/to/model      # HuggingFace model path (vLLM) or GGUF path
MAX_MODEL_LEN=65536                # Max context window
MAX_CONTEXT_TOKENS=45000           # Token budget per run
KV_CACHE_DTYPE=fp8                 # fp8 or auto
GPU_MEM_UTIL=0.92                  # vLLM GPU memory fraction
VLLM_PORT=6001                     # Inference server port
VLLM_PORTS=6001                    # Comma-separated for multi-GPU
TEMPERATURE=0.85
PRESENCE_PENALTY=1.1
MAX_WORKERS=1                      # Parallel workers (1 for single GPU)
ROLLOUT_COUNT=1                    # Number of answer rollouts

# === SearXNG (required for web search) ===
SEARXNG_URL=http://127.0.0.1:8080
USE_SEARXNG=true

# === Selenium (page reader) ===
SELENIUM_BROWSER=chrome
SELENIUM_HEADLESS=true
SELENIUM_TIMEOUT=30

# === Summary model (same server, used for page extraction) ===
API_KEY=dummy
API_BASE=http://127.0.0.1:6001/v1
SUMMARY_MODEL_NAME=/path/to/model  # Must match what /v1/models returns

# === Timeouts ===
OPENAI_CLIENT_TIMEOUT=7200

# === Visit tool ===
VISIT_SERVER_TIMEOUT=200
WEBCONTENT_MAXLENGTH=150000
VISIT_MAX_TOKENS=4000

# === CUDA ===
CUDA_HOME=/usr/local/cuda-12.8

# === Budget Controls ===
MAX_LLM_CALL_PER_RUN=50           # Max tool-call rounds
RUN_TIMEOUT_MINUTES=9999           # Hard timeout (minutes)
EARLY_STOP_MINUTES=0               # 0=disabled, N=force answer when N min remain
MAX_GENERATION_TOKENS=1500         # Agent: 1500 (concise), Report: 16384 (full)
